{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from finta import TA\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../algotrader2/resources/aapl_15min_indc_df.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our timestamp column as a datetime index, then save it as our index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X is everything except the new_signal column\n",
    "########\n",
    "# Dropping variables to fix negative loss.\n",
    "# Dropped: old signal, \n",
    "X = df.drop(['new_signal', 'pct_returns'], axis=1)\n",
    "\n",
    "# We should use the .shift() function so that our algorithm predicts the minute before realtime\n",
    "# Drop the row with NaN values \n",
    "X = X.shift().dropna()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the NEW signal column\n",
    "y = df[(\"new_signal\")]\n",
    "\n",
    "# Set start of training period\n",
    "training_begin = X.index.min()\n",
    "\n",
    "# we will train on 9 months and then test with the rest\n",
    "training_end = X.index.min() + DateOffset(months=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X_train and y_train DataFrames\n",
    "X_train = X.loc[training_begin:training_end]\n",
    "y_train = y.loc[training_begin:training_end]\n",
    "\n",
    "# Generate the X_test and y_test DataFrames\n",
    "X_test = X.loc[training_end:]\n",
    "y_test = y.loc[training_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler model to fit the X-train data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Transform the X_train and X_test DataFrames using the X_scaler\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "display(X_train_scaled.shape)\n",
    "display(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictors = len(X.columns)\n",
    "\n",
    "# We have 2 possible outcomes, and we are trying to predict the stock/indicators to be in position -1 or 1\n",
    "num_classes = 1\n",
    "\n",
    "num_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add dense layer(s)\n",
    "nn_model.add(Dense(units=32, input_dim=num_predictors, activation='relu'))\n",
    "nn_model.add(Dense(units=64, activation='relu'))\n",
    "nn_model.add(Dense(units=128, activation='relu'))\n",
    "nn_model.add(Dense(units=256, activation='relu'))\n",
    "nn_model.add(Dense(units=128, activation='relu'))\n",
    "nn_model.add(Dense(units=64, activation='relu'))\n",
    "nn_model.add(Dense(units=32, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-out layer(s)\n",
    "nn_model.add(Dropout(.2,input_shape=(10,)))\n",
    "\n",
    "# # Add dense layer, add Regularization\n",
    "# nn_model.add(Dense(5, activation='relu', kernel_regularized=l2(0.01), bias_regularized=l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer\n",
    "# Number of outputs equals number of classes\n",
    "nn_model.add(Dense(num_classes, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "nn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summarize model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "num_epochs = 100\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train,\n",
    "          epochs=num_epochs,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,     # This 'validation_split' is telling the neural network to keep 20% of the data to validate its score on the training set... this is to help AVOID OVERFITTING. \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model loss and accuracy\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back test\n",
    "# Predict values using testing data\n",
    "nn_test_predictions = nn_model.predict(X_test_scaled)\n",
    "nn_train_predictions = nn_model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to class labels (0 or 1) using 0.5 as the threshold\n",
    "nn_test_predictions_labels = (nn_test_predictions > 0.5).astype(int)\n",
    "nn_train_predictions_labels = (nn_train_predictions > 0.5).astype(int)\n",
    "\n",
    "# Training classification report\n",
    "train_class_report = classification_report(y_train, nn_train_predictions_labels)\n",
    "print(train_class_report)\n",
    "\n",
    "# Testing classification report\n",
    "test_class_report = classification_report(y_test, nn_test_predictions_labels)\n",
    "print(test_class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting how our accuracy dropped by so much when we ran the model on the testing data.\n",
    "The next iterations will begin to address the following:\n",
    "\n",
    "### FIRST: Let's get this algorithm running on the real market data; Paper Trading.\n",
    "- We'll have to find out how to link our model's predictions to our given paper trading brokerage account API. \n",
    "\n",
    "I this point, I believe that it is crucial we deploy our model on live market sessions before tuning it any further.\n",
    "\n",
    "### After the baseline deployment is observed:\n",
    "\n",
    "If it is clearly off during deployment, look deep into each step of the process to see if we are making a major mistake.\n",
    "\n",
    "If it seems to be making some sort of sense, then we can get tuning::\n",
    "- Train on more than 1 year's worth of OHLC data.\n",
    "- Think of using 3-second interval data gathered from TOS.\n",
    "- Use the SPY and NQ, and indicators such as VWAP and SMAs on the SPY and NQ, as variables to train our data on. \n",
    "    - A majority of stock \"follow\" the movement of these large indicies according to popular belief. AAPL is an interesting story because it is the largest stock in both indicies. So, we will also test this algorithm on other stocks and observe the numbers.\n",
    "- We can also possible combine the timeframes, and incorporate new timefimes; 1hr, 1d, 4hr, 30mins. We can put all timeframes and the indicators derived from the OHLC data give each timeframe to predict the pivot points on the 15 minute timeframe.\n",
    "- Tune our actual neural network algorithm (add more layers, drop out, etc.)\n",
    "- Try other ML algorithms such as SVM or Randomforests, then combine all ML algo's predictions to create a final prediction\n",
    "- Use CLOUD services to run EVERYTHING. \n",
    "- Automate our datamining process. \n",
    "- Implement sentiment analysis (TTVL chat, X, Reddit, emails)\n",
    "- Add options movement (Put/Call ratios, IV of puts/calls, other important variables)\n",
    "- Tune our indicators, add the indicators that we have not implemented yet that gave the error (\"cannot add multiple output columns to single column in df\")\n",
    "\n",
    "\n",
    "### The goal is consistient profitability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FUTURE USE\n",
    "\n",
    "# num_features = X_train_scaled.shape[1]\n",
    "# # Reshape the data to include the time steps dimension\n",
    "# # Here, we're using 1 time step as an example\n",
    "# X_train_reshaped = X_train_scaled.reshape(-1, 1, num_features)\n",
    "# X_test_reshaped = X_test_scaled.reshape(-1, 1, num_features)\n",
    "# # Reshape X_train_scaled and X_test_scaled\n",
    "# X_train_reshaped = X_train_scaled.reshape((num_samples, num_timesteps, num_features))\n",
    "# X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], num_timesteps, num_features))\n",
    "# display(X_train_reshaped)\n",
    "# display(X_test_reshaped)\n",
    "\n",
    "# num_timesteps = 1  # This is an example; you might need a different value\n",
    "# Change time step from 1 to more.\n",
    "# This can be calculated by dividing the total number of elements \n",
    "# in X_train_scaled by the number of features and then by the desired number of time steps.\n",
    "# Calculate the total number of samples that can be formed with 100 time steps\n",
    "# num_samples = X_train_scaled.shape[0] // 1000\n",
    "\n",
    "\n",
    "# # We have 2 possible outcomes, and we are trying to predict the stock/indicators to be in position -1 or 1\n",
    "# num_classes = 1\n",
    "# nn_model = Sequential()\n",
    "# # Add LSTM layer\n",
    "# nn_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "# nn_model.add(Dropout(0.2))\n",
    "# # Add another LSTM layer\n",
    "# nn_model.add(LSTM(units=50, return_sequences=False))\n",
    "# nn_model.add(Dropout(0.2))\n",
    "\n",
    "# # Add dense layers\n",
    "# nn_model.add(Dense(units=64, activation='relu'))\n",
    "# nn_model.add(Dropout(0.2))\n",
    "\n",
    "# # Compile model\n",
    "# nn_model.compile(loss=\"binary_crossentropy\",\n",
    "#               optimizer=\"adam\",\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Summarize model\n",
    "# nn_model.summary()\n",
    "\n",
    "\n",
    "# # Fit model\n",
    "# num_epochs = 50\n",
    "# nn_model.fit(X_train_reshaped, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FUTURE USE\n",
    "\n",
    "# # Save model history for further manipulation\n",
    "# model_history = model.history.model_history.keys()\n",
    "\n",
    "# # Now we can plot the accuracy for training and validation\n",
    "\n",
    "# training_results = pd.DataFrame(index=range(1, num_epochs+1))\n",
    "# training_results['Training'] = model_history['categorical_accuracy']\n",
    "# training_results['Validation'] = model_history['val_categorical_accuracy']\n",
    "# training_results.plot(title = 'Training and Validation Performance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
