{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have our scaled train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as JSON\n",
    "nn_json = nn.to_json()\n",
    "\n",
    "file_path = (\"../Resources/model.json\")\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json_file.write(nn_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "file_path = \"../Resources/model.h5\"\n",
    "nn.save_weights(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to transfer learning\n",
    "\n",
    "Say we wanted to use a previous model and train it with more data, without having to run the whole model over again. Consider if it took days to train our model? We would want to save that and then add to it. The following is how we would do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "file_path = (\"../Resources/model.json\")\n",
    "with open(\"../Resources/model.json\", \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "loaded_model = model_from_json(model_json)\n",
    "\n",
    "# load weights into new model\n",
    "file_path = \"../Resources/model.h5\"\n",
    "loaded_model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `layers` attribute stores the various layers in the model as a list\n",
    "loaded_model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in new data but work with OLD the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some new data\n",
    "X = pd.read_csv('../Resources/meet_or_beat_US_new_data.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = loaded_model.predict(X)\n",
    "new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('../Resources/meet_or_beat_AU.csv')\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing windows\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_var = 'after_total_returns'\n",
    "x_vars = list(new_data.columns)\n",
    "x_vars.remove(y_var)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data[x_vars], new_data[y_var], random_state=1)\n",
    "\n",
    "# Create the StandardScaler instance\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale both the training and testing data from the features dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the existing layers of the loaded model\n",
    "for layer in loaded_model.layers[0:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can add new layers to existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DNN to hold the old one\n",
    "transfer_model = Sequential()\n",
    "# Go through each layer, skipping the last layer\n",
    "for layer in loaded_model.layers[:-1]: \n",
    "    transfer_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an additional layer\n",
    "transfer_model.add(Dense(10, activation=\"relu\"))\n",
    "# Add the final output layer\n",
    "transfer_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "transfer_model.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "transfer_model.fit(X_train_scaled,y_train, \n",
    "                    epochs=20,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
