{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's determine the stocks of which we'd like to pull the OHLC data.\n",
    "Let's go with all of the stocks in the NQ and the ES. This should serve as a nice basis for how the type of stocks that we want to deploy our algorithm onto move; ***mega-cap stocks that offer range-bound trading during periods of low volatility.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of the stocks in the NQ and ES onto a dataframe. \n",
    "\n",
    "Let's start with the NQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read csv, take a look\n",
    "# nq_df = pd.read_csv((\"../algotrader2/resources/nq_stocks.csv\"))\n",
    "# nq_df.head()\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the same for the snp 500 symbols\n",
    "# snp_df = pd.read_csv((\"../algotrader2/resources/snp_stocks.csv\"))\n",
    "# snp_df.head()\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the symbols from each df\n",
    "# nq_symbols_df = nq_df[\"Symbol\"]\n",
    "# snp_symbols_df = snp_df[\"Symbol\"]\n",
    "\n",
    "# # Put dfs together\n",
    "# snp_nq_symbols_df = pd.concat([nq_symbols_df, snp_symbols_df], axis=0)\n",
    "\n",
    "# # Drop duplicates\n",
    "# snp_nq_symbols_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# # View\n",
    "# snp_nq_symbols_df\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert symbols to a list so that we can feed it into our Alpaca API.\n",
    "# # Alpaca is where we will access the historical OHLC data needed.\n",
    "\n",
    "# nq_snp_symbols_list = snp_nq_symbols_df.tolist()\n",
    "\n",
    "# len(nq_snp_symbols_list)\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FOLLOWING IS WHEN WE CONTINUE WITH MANY STOCKS...\n",
    "# # THE FOLLOWING NEEDS TO BE ITERATED\n",
    "\n",
    "# # Rearrange df\n",
    "# # Separate ticker data\n",
    "\n",
    "# # WE'LL HAVE TO iterate the below for all stocks in both the SPY and NQ\n",
    "\n",
    "# AAPL = aapl_msft_df[aapl_msft_df['symbol']=='AAPL'].drop('symbol', axis=1)\n",
    "# MSFT = aapl_msft_df[aapl_msft_df['symbol']=='MSFT'].drop('symbol', axis=1)\n",
    "\n",
    "# AAPL.sort_index(inplace=True)\n",
    "# MSFT.sort_index(inplace=True)\n",
    "\n",
    "# # Concatenate the ticker DataFrames\n",
    "# am_df = pd.concat([AAPL, MSFT],axis=1, keys=['AAPL','MSFT'])\n",
    "\n",
    "\n",
    "# am_df\n",
    "\n",
    "\n",
    "# # FEATURE CREATION WITH TWO stocks... this is for LATER USE\n",
    "\n",
    "# # Generate returns from close column with pct_change\n",
    "\n",
    "# # WILL NEED TO INTERATE\n",
    "# am_df[(\"AAPL\",\"actual_returns\")] = am_df[(\"AAPL\",\"close\")].pct_change()\n",
    "# am_df[(\"MSFT\",\"actual_returns\")] = am_df[(\"MSFT\",\"close\")].pct_change()\n",
    "\n",
    "# # Drop NaN\n",
    "# am_df = am_df.dropna()\n",
    "\n",
    "# # Simple Moving Averages\n",
    "# am_df[(\"AAPL\",\"AAPL_SMA5\")] = TA.SMA(am_df[\"AAPL\"],5)\n",
    "# am_df[(\"AAPL\",\"AAPL_SMA10\")] = TA.SMA(am_df[\"AAPL\"],10)\n",
    "\n",
    "# am_df[(\"MSFT\",\"MSFT_SMA5\")] = TA.SMA(am_df[\"MSFT\"],5)\n",
    "# am_df[(\"MSFT\",\"MSFT_SMA10\")] = TA.SMA(am_df[\"MSFT\"],10)\n",
    "\n",
    "# # Exponential Moving Averages\n",
    "# am_df[(\"AAPL\",\"AAPL_EMA3\")] = TA.EMA(am_df[\"AAPL\"],3)\n",
    "\n",
    "# am_df[(\"MSFT\",\"MSFT_EMA3\")] = TA.EMA(am_df[\"MSFT\"],3)\n",
    "\n",
    "# # NEEDS TO BE ITERATED!!\n",
    "\n",
    "\n",
    "# # Create a df with all the indicators for both AAPL and MSFT\n",
    "# aapl_indicator_df = am_df[\"AAPL\"].drop([\"actual_returns\"], axis=1)\n",
    "# msft_indicator_df = am_df[\"MSFT\"].drop([\"actual_returns\"], axis=1)\n",
    "\n",
    "# # Save the signal column as our 'y'\n",
    "# y=am_df[\"AAPL\"][\"signal\"]\n",
    "\n",
    "# display(aapl_indicator_df.head())\n",
    "# display(aapl_indicator_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now let's set up our alpaca API to get the OHLC data.\n",
    "\n",
    "\n",
    "### Alpaca API for Historical OHLC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Alpaca API, .env files, requests\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "# Verify that Alpaca key and secret were correctly loaded\n",
    "print(f\"Alpaca Key type: {type(alpaca_api_key)}\")\n",
    "print(f\"Alpaca Secret Key type: {type(alpaca_secret_key)}\")\n",
    "\n",
    "# Create the Alpaca API object\n",
    "alpaca = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and end dates and specify isoformat\n",
    "# We will start off with on week in the middle of November of this year, 2023.\n",
    "\n",
    "start_date = pd.Timestamp(\"2023-01-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(\"2024-01-17\", tz=\"America/New_York\").isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tickers\n",
    "# Test with one for now.\n",
    "# Later, this is where we will feed in our list of nq/es stocks we created above\n",
    "tickers = [\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timeframe for Alpaca API\n",
    "timeframe = \"1Min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical OHLC data for AAPL\n",
    "aapl_df = alpaca.get_bars(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = end_date\n",
    ").df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "aapl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR NOW, we will not include pre-market/after-hours action, but WE WILL in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ticker data\n",
    "AAPL = aapl_df[aapl_df['symbol']=='AAPL'].drop('symbol', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the indexes\n",
    "\n",
    "AAPL.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the ticker DataFrames\n",
    "# AAPL = pd.concat([AAPL, MSFT],axis=1, keys=['AAPL','MSFT'])\n",
    "\n",
    "# # Preview DataFrame\n",
    "# AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Now we have the OHLC data here in our workspace, and we can move onto the next step of the process; data cleaning.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Drop NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "AAPL = AAPL.dropna()\n",
    "\n",
    "AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1 minute: Dropping the NaN just took out about 600 rows of data. Did it take out the whole column given the date? That is not good, since if we have the data for one stock at a given minute but not the other, we don't want to throw out that info for the one stock. \n",
    "\n",
    "3166 to 2405 rows after dropping NaN.\n",
    "\n",
    "We will come back to this. Remember, we don't want little things like this from holding us back from implementing this. Just make a note of these little issues and crush them during the next iteration. \n",
    "\n",
    "UPDATE 1: With 3 minutes, we start with 1200 rows, then have 1014 after dropping NaN. \n",
    "\n",
    "Seems to get better when we zoom out... We will most probably just source better data, however. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this cleaned dataframe to a new .csv\n",
    "AAPL.to_csv('../algotrader2/Resources/aapl_1min_df.csv', index=\"True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to generate the features from the data set that we will train our model with. \n",
    "\n",
    "[Feature Creation->](/Users/montygash/Desktop/ETAlgo/tradingbot/nn_feature_creation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Is this how we want the algorithm to be? To only train itself on a particular stock? \n",
    "    - No... I want it to be trained on MULTIPLE stocks. So it is trained on both MSFT, AAPL, and even TSLA, AMD, NVDA... and then I deploy it on any stock that I want to deploy it onto. \n",
    "    - This is a major issue that I need to resolve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This begs another question: Do we only need to shift it back one? Since we are using 1 minute candles? Maybe we should be shifting it back more than one one minute bar?... We will revisit this later.\n",
    "\n",
    "- Is this how we want the algorithm to be? To only train itself on a particular stock? \n",
    "    - No... I want it to be trained on MULTIPLE stocks. So it is trained on both MSFT, AAPL, and even TSLA, AMD, NVDA... and then I deploy it on any stock that I want to deploy it onto. \n",
    "    - This is a major issue that I need to resolve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 MINUTE\n",
    "# Set timeframe for Alpaca API\n",
    "timeframe = \"3Min\"\n",
    "\n",
    "# Get historical OHLC data for AAPL\n",
    "aapl_3min_df = alpaca.get_bars(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = end_date\n",
    ").df\n",
    "\n",
    "aapl_3min_df\n",
    "\n",
    "# Separate ticker data\n",
    "aapl_3min_formatted = aapl_3min_df[aapl_3min_df['symbol']=='AAPL'].drop('symbol', axis=1)\n",
    "\n",
    "# Drop NA\n",
    "aapl_3min_formatted = aapl_3min_formatted.dropna()\n",
    "\n",
    "display(aapl_3min_formatted.head())\n",
    "\n",
    "# Now save this cleaned dataframe to a new .csv\n",
    "aapl_3min_formatted.to_csv('../algotrader2/Resources/aapl_3min_df.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 MINUTE\n",
    "# Set timeframe for Alpaca API\n",
    "timeframe = \"15Min\"\n",
    "\n",
    "# Get historical OHLC data for AALP and MSFT\n",
    "aapl_15min_df = alpaca.get_bars(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = end_date\n",
    ").df\n",
    "\n",
    "aapl_15min_df\n",
    "\n",
    "# Separate ticker data\n",
    "aapl_15min_formatted = aapl_15min_df[aapl_15min_df['symbol']=='AAPL'].drop('symbol', axis=1)\n",
    "\n",
    "\n",
    "# Drop NA\n",
    "aapl_15min_formatted = aapl_15min_formatted.dropna()\n",
    "\n",
    "display(aapl_15min_formatted.head())\n",
    "\n",
    "# Now save this cleaned dataframe to a new .csv\n",
    "aapl_15min_formatted.to_csv('../algotrader2/Resources/aapl_15min_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we will divide the code up strategically, so that we can dive into each moving part and tune it as needed. Our model should become more and more sofisticated and accurate over future iterations until our goal of consistient profitability is met."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
