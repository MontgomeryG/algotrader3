{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we must define our goal.\n",
    "\n",
    "*UNDERSTANDING OUR PROBLEM:*\n",
    "- We need capital for future investments. The emotional state the above situation creates severely inhibits a trader's ability to become consistiently profitable.\n",
    "\n",
    "- Two other key factors that we are seeking to overcome with this project:\n",
    "    - Lack of discipline. Leads traders to make trades that go against their proven process. \n",
    "    - No a daily max loss shut down, a tool that prevents over trading into a deeper monetary drawn-down.\n",
    "\n",
    "\n",
    "### THEREFORE:\n",
    "- I want to create a trading bot(s) with neural networks that trades for me.\n",
    "\n",
    "- I will feed this bot as much data as possible, with the refrence point of which independent variables I choose to feed it will be partially dependent upon what I have been using to trade for these past few years. However, since I have not been profitable using those signals, I will also include other variables that I have learned to be important to other traders in the market.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Here are a few important questions:\n",
    "\n",
    "*What* is this bot going to trade?\n",
    "\n",
    "*When* is this bot going to trade?\n",
    "\n",
    "How *frequently* is the bot going to trade (number of executions given timeframe)?\n",
    "\n",
    "(Added After Initial Write-Up) Consideration of *multiple* bots? Different tools for different jobs.\n",
    "\n",
    "\n",
    "### Initial thoughts in response to the above questions:\n",
    "- Stocks move in circles... so play something where there is no complete direction?\n",
    "    - (No, that doesn't seem right. The break-ups and break-downs we want to capture and hold...sometimes stocks breakdown and never go back to that price; shorting major breakdown levels on weak stocks is a real strategy.) \n",
    "- It is going to be tricky for us to determine how this bot wants to trade. \n",
    "    - Do we want it to try scalping within ranges, or try to catch breakouts? \n",
    "    - Ideally, we don't want to make that decision. We want to employ a bot that itself decides when to buy, sell, hold, short, and cover a stock on an intraday basis. \n",
    "- We want it to be able to run on stocks with a lot of liqudity, like mega-cap stocks such as TSLA, AMZN, AMD...but we also shouldn't forgoe the mass opportunity that exist on a weekly basis of small-cap stocks with fresh catalysts that breakout and can go up 100s of percents.\n",
    "- I want the bot to decide how it trades these types of stocks. Maybe it would be a good idea to seperate the categories of stocks that the bot is trained on, large cap vs small cap. Is doing so even necessary?\n",
    "    - (Yes, I believe doing so is necessary. Different tools for different jobs seems to be a good guiding idea.)\n",
    "\n",
    "**Next, let's brainstorm what we believe to be the most important variables when trading stocks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING VARIABLES:\n",
    "\n",
    "- Time and Sales(price, size, exchange?, time), speed and acceleration of the time and sales (the bot should be able to pick up on that..(?))\n",
    "    - We may have to create speed and acceleration as new input features.\n",
    "    - Note, the time and sales is not the same as the Open High Low Close.\n",
    "- By feeding in time and sales, we will capture the volume at given period of time... so we will have the levels mapped in the algo intrinsically. \n",
    "- volatility.... wouldn't this be intrinsically included with the historical time and sales data? Yes\n",
    "- Historical volume... wouldn't this be intrinsically included with the historical time and sales data? Yes\n",
    "\n",
    "- VWAP, Anchored VWAP\n",
    "- Full Level 2 (or as much as possible.)\n",
    "- Options activity(maybe only on the prices that are \"close\" to current price, becuase otherwise it will be a lot of data...)\n",
    "    - Options chain is very important. I like the idea of including at least the nearby strike prices.\n",
    "    - Implied Volatility. Maybe just implied volatility instead of the individual options, if those options are too much data to handle.\n",
    "    - Implied volatility is the Standardized measure of the prices of all options trading on a given stock.\n",
    "- 20, 50, 200 period moving averages... why not just throw in more?? 10 SMA, 5 SMA.. Then the machine will determine which are most important?\n",
    "    - Same thing with Exponential Moving Averages, double, triple...\n",
    "- Sector, what other stocks are doing in the same sector, and overall movement of other sectors\n",
    "- News. (see sentiment_analysis_addition.ipynb for more info)\n",
    "    - Incorporating News catalysts/headlines in the way we would like to do is going to be a long process to incorporate news, so we will likely omit it at the beginning\n",
    "    - We would first need to go in-depth on our definitions of which news headlines are actually \"something\"...so we would introduce the sentiment analysis here.\n",
    "    - \"Scraping doesn't work that well. They don’t want you to scrape… \" \n",
    "    - So we could subscribe to company email list, have their headlines in our email as soon as they release, then scrape the text that they provide.\n",
    "    - [More Info on addition of Sentiment Analysis->](sentiment_analysis_addition.ipynb)\n",
    "- Short interest, days to cover\n",
    "- Easy or hard to borrow.\n",
    "- Market on open/close Imbalances. \n",
    "- Psychological levels... $0.25, $0.5, $0.75 (algo should pick up on this... we aren't going to explicitly tell it to put more weight on these levels.)\n",
    "- Candle sticks on multiple timeframes\n",
    "- Wicks on those candle sticks? Traders always use wicks to make decisions. Will the algo be able to just make good decisions? Or would We have to program in a function that captures the \"wicks\" and then we feed the magnitude of the wick on the given time frame to the ML algo.\n",
    "- Fibonaccis.\n",
    "- Shares available to trade\n",
    "- Institutional ownership,\n",
    "- Insider buying(yes or no? How much as a percentage of shares available to trade?)\n",
    "- Market capitalization\n",
    "- Whales buying(\"Unusual Whales\" Website)...SEC filings\n",
    "- Support and resistance levels... I want the algorithm to make it's own based on what it feels is most important.\n",
    "- What about the percentage gains of some of the mega-cap stocks/market leading stocks...  or is that too much and not needed?\n",
    "- What about the movement of crypto?\n",
    "- Fear/Greed index\n",
    "\n",
    "\n",
    "**(Later Additions)**\n",
    "- Why not just put as many indicators as possible (check out the FinTA library), and then we let the machine decide the most important, and then we cut the unneeded ones? This sounds like a good plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Important Ideas\n",
    "\n",
    "- March 2023 Bank Run: Silicon Valley Bank (SIVB) shareholders in March 2023. SIVB, 1 share at halted pending news on March 10th around $30... the stock opened at around 0.10 about 2 weeks later.\n",
    "    - One person I connected with online said that they had a big short position, thousands of shares right before SIVB when into a halt. The short was a good idea for the majority of the day since the company was failing, BUT, it went into a NEWS PENDING HALT around the middle of the day, as the FED sorted things out. SIVB stayed halted for about a WEEK or 2. So, think about the person who was short a few thousand shares when SIVB halted at $30. They were going in for a quick scalp trade(a trade lasting seconds to a few minutes), and then got put in a position where they were *completely at mercy of the market*. They had thousands of dollars tied up for weeks, not knowing if SIVB would be bailed out, resulting in the stock price going up most likely, or if SIVB would fail completely. Luckily, when the stock opened up below $1, that person counted their blessings and made a lot of money. We need to be able to put in safe guards for instances like this. If we were to take the other side of that person's trade, we would have been smoked. If we ever get close to a halt we want to get out... right?\n",
    "- How do we address backtesting biases?\n",
    "    - Survivorship Bias: not accounting for the stocks that did not make\n",
    "    - Small selection of stocks bias.\n",
    "- Diversification is important. We want to be in multiple stocks that are looking like the best options.\n",
    "- Stocks that have high ATR.(Average True Range)    \n",
    "- Historical data on level 2 action? Seems like a lot of data... will we have to pay? \n",
    "    - Neal Roberts, a respected prop trader, put emphasis on the fact that there was a 160k offer on CCL at one point in time... so, size on the level 2 is very important. That is what people are looking at... Right? Well, we have to get the data. Does that data exist?  \n",
    "- Define risk/rewawrd\n",
    "- Variable stop losses\n",
    "- Limit order vs. Market Order\n",
    "- Impact of other Algos, High Frequency trading algos, HFTs\n",
    "- Hold Time: We want the hold time to be variable based on what the market is showing. The machine shall dictate the hold time...\n",
    "    - Maybe we can test to see whether it is more profitable to exit the position fully in pofit, or allow the machine to incrementally scale out of the position, similar to how the pro/proprietery traders do... But, they only do so because it makes it easier to hold their position because of their EMOTIONS. these emotions are not something we will have to worry about when deploying our algorithm. \n",
    "- When fundamental market changing news comes out, we will probably want to exit the trade...(unless we build an algorithm to specifically play those market moving events)\n",
    "- We want to trade stocks in play. Fresh catalysts lead to more liqudity which allows for scalibility. \n",
    "- Remember that relative strenght/weakness can show big buyer/sellers. We can ride the wave they create as then enter/exit their position. Keep in mind that these big players are using more zoomed-out timeframes.\n",
    "- Must account for pre/post-market price action.\n",
    "- Maybe just deploy the algo onto one stock at a time. Would it be too much to pull data on and then place the algo onto a bunch of different stocks at the same time?\n",
    "\n",
    "\n",
    "*As we can see, there is a lot to consider. We don't want to get overwhelmed though, so let's begin on what the implementation will look like. We are going to keep things simple at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Break Down\n",
    "\n",
    "1) Get data. Prep data. Drop NA. Drop irrelevant columns. Scale data.\n",
    "2) Feature engineering. Create more features out of data. Take the close column and make, percent change, simple moving averages, VWAP.\n",
    "3) Split the data for training and testing. Normalize and clean and prep data with StandardScaler.\n",
    "4) Modeling. Hyperparameter tuning. Set parameters within functions (check manual for each function to see which parameters we can tune). Change number of layers. Set the learning rate. Drop-out. \n",
    "5) Fit the model. \n",
    "6) Predict function. \n",
    "7) Check model performance: Accuracy. Loss. Error. Confusion metrics. Classification report(precision, recall, F1-Score). Area under the curve.\n",
    "8) Deploy model. Monitor performance.\n",
    "9) Improve model. (Back to 2)\n",
    "\n",
    "\n",
    "Below contains sample code of what we will be using in the rest of the project. It was copied directly from the solved activities within the UCBE Fintech Bootcamp. This is just a template for our actual implementation; it is not meant to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Insctuctor Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Features (basic implementation using short and long moving averages)\n",
    "\n",
    "Note the use/necessity of the '.shift()' function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually Compare the Actual and Predicted Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictions DataFrame\n",
    "predictions_df = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "predictions_df[\"predicted_signal\"] = testing_signal_predictions\n",
    "\n",
    "predictions_df[\"actual_returns\"] = trading_df[\"actual_returns\"]\n",
    "\n",
    "predictions_df[\"trading_algorithm_returns\"] = (\n",
    "    predictions_df[\"actual_returns\"] * predictions_df[\"predicted_signal\"]\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live trading with Alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you get your .env set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "API_SECRET = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "ALPACA_API_BASE_URL = \"https://paper-api.alpaca.markets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the API \n",
    "api = tradeapi.REST(API_KEY, API_SECRET, ALPACA_API_BASE_URL, api_version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set signal variable\n",
    "signal = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buy signal, num shares and ticker\n",
    "if signal == 1:\n",
    "    orderSide = \"buy\"\n",
    "else:\n",
    "    orderSide = \"sell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ticket symbol and the number of shares to buy\n",
    "ticker = \"TSLA\"\n",
    "number_of_shares = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API call\n",
    "prices = api.get_bars(ticker, \"1Min\").df\n",
    "\n",
    "# Reorganize the DataFrame\n",
    "prices = pd.concat([prices], axis=1, keys=[\"TSLA\"])\n",
    "\n",
    "# Get final closing price\n",
    "limit_amount = prices[\"TSLA\"][\"close\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we submit the order. This is only for Alpaca, and we will need to update our API call to work with whatever brokerage we are working with. We will revisit it when needed, and take it slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit order\n",
    "api.submit_order(\n",
    "    symbol=\"TSLA\", \n",
    "    qty=number_of_shares, \n",
    "    side=orderSide, \n",
    "    time_in_force=\"gtc\", \n",
    "    type=\"limit\", \n",
    "    limit_price=limit_amount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, here's an implementation by ChatGPT 3.5: [GPT implementation and breakdown->](chat_gpt_implementation.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "Good!\n",
    "\n",
    "So that's the basis of what we are trying to do. Which model will we use? Well, which one will work the best? Maybe we use all three together? Isn't neural networks *lightyears* ahead of these older ML models? \n",
    "\n",
    "Now the fun begins. There are so many variables and questions to be answered. It is a good idea to take it slow, let the ideas flow, and seek to collaborate. Try not to take the resources you have at your advantage for granted. Keep going.\n",
    "\n",
    "\n",
    "### Implementation Break Down\n",
    "\n",
    "1) Get data. Prep data. Drop NA. Drop irrelevant columns. Scale data.\n",
    "2) Feature engineering. Create more features out of data. Take the close column and make, percent change, simple moving averages, VWAP.\n",
    "3) Split the data for training and testing. Normalize and clean and prep data with StandardScaler.\n",
    "4) Modeling. Hyperparameter tuning. Set parameters within functions (check manual for each function to see which parameters we can tune). Change number of layers. Set the learning rate. Drop-out. \n",
    "5) Fit the model. \n",
    "6) Predict function. \n",
    "7) Check model performance: Accuracy. Loss. Error. Confusion metrics. Classification report(precision, recall, F1-Score). Area under the curve.\n",
    "8) Deploy model. Monitor performance.\n",
    "9) Improve model. (Back to 2)\n",
    "\n",
    "\n",
    "\n",
    "Let's go more clearly define our training variables so that we can determine which data we need to obtain. (training_variables.ipynb) [Next->](training_variables.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
